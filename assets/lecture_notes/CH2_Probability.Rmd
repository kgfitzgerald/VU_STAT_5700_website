---
title: "Chapter 2: Probability"
subtitle: "STAT 5700: Probability"
author: "Prof. Katie Fitzgerald, PhD"
institute: "Villanova University Department of Mathematics & Statistics"
date: "Fall 2025"
output: 
  pdf_document:
    includes:
      in_header: preamble.tex
    toc: true
editor_options: 
  markdown: 
    wrap: 72
---

```{r, echo = FALSE, message=FALSE, warning = FALSE}
library(tidyverse)
```

\pagebreak

# 2.1 Introduction

**What is Probability?**

Colloquially, a measure of one's belief in the occurrence of a future event.

Probability is a mathematical framework to describe and analyze random phenomenon.

**Okay.. so what are random phenomenon?**

Anything we cannot predict with certainty

-   e.g. weather, score of a soccer game, score on a test, arrival time
    of a flight, protection of vaccine, whether your roommate will wash
    the dishes, etc
    
Concept of probability is necessary in work with physical, biological, or social mechanisms that generate observations that cannot be predicted with certainty.

An alternative definition: probability is the mathematics of uncertainty.

Even if random events can't be predicted with certainty, the relative frequency with which they occur in a long series of trials is often remarkably stable.

### A goofy (and absurd) example: Random babies

::: {.activitybox data-latex=""}

*Courtesy of Allan Rossman (2019)*

Imagine that a hospital returns newborn babies to their mothers at
random... 

Use your intuition to arrange the following events in order, from least
likely to most likely:

A.   None of the four mothers gets the correct baby.
B.   At least one of the four mothers gets the correct baby.
C.   All of the four mothers gets the correct baby.

How should we investigate the real order? Start by simulating...

1.  Write the first names of 4 babies on your 4 index cards
2.  Take a blank sheet of paper and divide it into four sections
3.  Write the last name of a mother in each section. Make sure you know
    which baby matches to which mother - try using alliterations (e.g.
    Bob Barker)
4.  Shuffle the cards (babies) and randomly distribute them to the
    sections of the sheet (mothers)
5.  How many matches occurred? Write this number down
6.  Repeat Steps 4-5

:::

\pagebreak

::: {.activitybox data-latex=""}

### Random babies (cont'd)


**Class by-hand Simulation results:**

|Matches| Count \hspace{0.4in}| Fraction \hspace{0.4in}| Decimal |
|------|------|------|------|
|0| | | | 
|1| | | | 
|2 | | | | 
|3 | | | | 
|4 | | | | 

**Computer simulations**

Let's do this *many* more times: 

<http://www.rossmanchance.com/applets/randomBabies/RandomBabies.html>

+ Determine the approximate probability that **at least one** mother gets
the correct baby. 
    + Indicate two different ways to determine this. 
+ Interpret this (approximate) probability.\
\
\


If $P(A)$ = the long-run proportion of times event $A$ will occur, can
we determine that exact limiting value?

First need to determine all the possible outcomes:

```{r, echo = FALSE, fig.align="center", out.width="90%"}
knitr::include_graphics("./images/random_babies_S.png")
```

+ Determine the number of matches for each outcome.  
+ Then count how many outcomes produce 0 matches, 1 match, and so on.  
+ Finally, divide by the total number of outcomes to determine the exact probabilities. Express these probabilities as fractions and also as decimals, with three decimal places of accuracy


|Matches| Count \hspace{0.4in}| Fraction \hspace{0.4in}| Decimal |
|------|------|------|------|
|0| | | | 
|1| | | | 
|2 | | | | 
|3 | | | | 
|4 | | | | 

:::

::: {.activitybox data-latex=""}

+ What is the exact (theoretical) probability that at least one mother gets the correct baby?
\
\

+ Finally, let's return to our original question... order these from least
likely to most likely:

  A.   None of the four mothers gets the correct baby.
  B.   At least one of the four mothers gets the correct baby.
  C.   All of the four mothers gets the correct baby.
\
:::

### So, what is Probability?

\textcolor{red}{\textbf{Probability}} = the proportion of outcomes of a
random experiment that terminate in the event A as the number of trials
of that experiment increases without bound. 

If an experiment is repeated $n$ times under essentially identical
conditions, and if the event $A$ occurs $n_A$ times, then as $n$ grows
large, the ratio $n_A/n$ approaches a fixed limit that is the
probability of event $A$. That is, $P(A) = n_A/n$

:::: {.highlightbox data-latex=""}
::: {.center data-latex=""}
Probability interpreted as the proportion of times an event occurs in
the long run
:::
::::

# 2.2 Probability & Inference

### What does probability have to do with inference?

Consider a gambler who wishes to make an inference about the balance of a six-sided die. 

Hypothesis: the die is balanced (all 6 sides equally likely to occur; not weighted).

Wants to use real-world observations to evaluate the hypothesis (reject the theory if false). 

His data: 10 rolls of the die, all 1s. Rejects the hypothesis because even though ten 1s not an *impossible* result, he judges it to be an *improbable* one. 

His judgement is probably based on intuition. 

What if he got five 1s, and a mix of other numbers? Is that highly improbable? Or four 1s? 

Many events are in a grey area that intuition alone makes it difficult to guess the probability. We need a rigorous method for finding a number (a probability) that will agree with the actual relative frequency of occurrence of an event in a long series of trials. 

We need a **theory of probability** that will permit us to calculate the probability of observing specified outcomes, assuming that our hypothesized model is correct.

Set theory is a first set of tools that will help us in constructing probabilistic models for experiments. 

\pagebreak

# 2.3 A Review of Set Notation

We use capital letters, $A, B, C$, etc to denote sets of points. E.g., if $a_1, a_2, a_3$ are elements in the set $A$, we write $$A = \{a_1, a_2, a_3\}$$

### Set Theory / Activity

```{r, echo = FALSE, out.width="65%", out.height="30%", fig.align="center"}
knitr::include_graphics("./images/1.1-1_venn_diagram.png")
```

### Notation / terminology

$A \cup B$ is the \textcolor{red}{\textbf{union}} of $A$ and $B$

> Union = event $A$ occurs OR event $B$ occurs OR both

$A \cap B$ is the \textcolor{red}{\textbf{intersection}} of $A$ and
    $B$

> Intersection = event $A$ occurs AND event $B$ occurs

$\emptyset$ denotes the \textcolor{red}{\textbf{null}} or
    \textcolor{red}{\textbf{empty}} set

$A'$ is the \textcolor{red}{\textbf{complement}} of $A$ (our book uses $\overline{A}$, others use $A^c$)

> i.e. all elements in $S$ that are not in $A$

$A \subset B$ means $A$ is a \textcolor{red}{\textbf{subset}} of $B$ (all elements in $A$ are also in $B$)

\textcolor{red}{\textbf{Mutually Exclusive}} =
    \textcolor{red}{\textbf{Disjoint}} = Events that cannot occur
    simultaneously
$$A\cap B = \emptyset$$
\textcolor{red}{\textbf{Mutually Exhuastive}}= a set of events that completely cover the sample space

$$A\cup B \cup C= S$$

::: {.activitybox data-latex=""}

**Example**: 

> Event A = stock market closes up today

> Event B = stock market closes down today

> A and B cannot both happen $\implies$ A & B are disjoint

```{r, echo = FALSE, out.width="10%", out.height = "5%", fig.align="center"}
knitr::include_graphics("./images/venn_mutually_exclusive.png")
```

:::



::: {.activitybox data-latex=""}

**Example**: 

> Event A = patient's temperature increases

> Event B = patient's temperature decreases

> Event C = patient's temperature stays the same

> $\{A,B,C\}$ encompasses all posibilities $\implies$ mutually exhaustive

:::

### Laws of Set Algebra

::: {.highlightbox data-latex=""}

**Commutative Laws**

$$\begin{aligned}
A \cup B &= B \cup A \\
A \cap B &= B \cap A
\end{aligned}$$

:::

::: {.highlightbox data-latex=""}

**Associative Laws**

$$\begin{aligned}
(A \cup B) \cup C &= A \cup (B \cup C) \\
(A \cap B) \cap C &= A \cap (B \cap C)
\end{aligned}$$

:::

::: {.highlightbox data-latex=""}

**Distributive Laws**

$$\begin{aligned}
A \cap (B \cup C) &= (A \cap B) \cup (A \cap C) \\
A \cup (B \cap C) &= (A \cup B) \cap (A \cup C)
\end{aligned}$$

:::

::: {.highlightbox data-latex=""}

**DeMorgan's Laws**

$$\begin{aligned}
(A \cup B)' &= A' \cap B' \\
(A \cap B)' &= A' \cup B'
\end{aligned}$$

:::

# 2.4 A Probabilistic Model for an Experiment: The Discrete Case

### Some definitions

\textcolor{red}{\textbf{Experiment}} = process by which an observation is made.

\textcolor{red}{\textbf{Random experiment}} = process of observing the outcome of a chance event (i.e. the outcome of the experiment cannot be predicted with certainty).

> Random babies: process of randomly delivering 4 babies to 4 mothers (1 shuffle in our simulation = 1 random experiment)

Experiments can result in one or more outcomes, which are called \textcolor{red}{\textbf{events}}, denoted by capital letters (eg. A, B)

> Random babies example outcomes/events:

+ A: At least one baby delivered to its correct mother
+ B: Less than 4 babies delivered to their correct mothers
+ $E_0$: 0 babies delivered correctly
+ $E_1$: Exactly 1 baby delivered correctly
+ $E_2$: Exactly 2 babies delivered correctly
+ $E_4$: All 4 babies delivered correctly

Note that events can either be **compound** or **simple**. For example, if you observe $A$ (*at least one baby*), you will have also observed either $E_1, E_2$, or $E_4$. $A$ is an example of a compound event, because it can be decomposed into other simple events. 

A \textcolor{red}{\textbf{simple event}} is an event that cannot be decomposed. Each simple event corresponds to one and only one sample point. 

\textcolor{red}{\textbf{Sample space}} = the set of all possible simple outcomes/events for an experiment

> Random babies: {0,1,2,4 matches}

> Sample space = S (or sometimes $\Omega$)

\textcolor{red}{\textbf{Discrete sample space}} is one that contains either a finite or countable number of distinct sample points.

An *event* in a discrete sample space $S$ is a collection of sample points - that is, any subset of $S$. 


### What is Probability? (more formally)

::: {.highlightbox data-latex=""}

**Definition 2.6** Suppose $S$ is a sample space associated with an experiment. To every event $A$ in $S$ ($A$ is a subset of $S$), we assign a number $P(A)$, called the \textcolor{red}{\textbf{probability}} of $A$, so that the following axioms hold:  

1. $P(A) \geq 0;$
1. $P(S) = 1;$ 
1. if $A_1, A_2, A_3, ...$ are a series of pairwise mutually exclusive events (i.e., 
$A_i \cap A_j = \emptyset,$ if $i \neq j$), then
$$P(A_1 \cup A_2 \cup ... \cup A_k) = P(A_1) \cup P(A_2) \cup ... \cup P(A_k) = \sum_{i = 1}^kP(A_i)$$
for each positive integer $k$, and
$$P(A_1 \cup A_2 \cup A_3 \cup ... ) = P(A_1) \cup P(A_2) \cup P(A_3) \cup ... = \sum_{i = 1}^\infty P(A_i)$$
for an infinite, but countable, number of events

:::

Note: the definition states only the conditions an assignment of probabilities must satisfy; it does not tell us *how* to assign specific probabilities of events.

Example: Suppose a coin yielded 800 heads in 1000 tosses. Consider the experiment of one more toss of the same coin. 

+ Two possible outcomes (simple events): heads or tails
+ Definition of probability allows us to assign to these two simple events any two non-negative numbers that add to 1.
+ Could do 1/2 for each simple event.
+ Past data suggests P(heads) = 0.8 and P(tails) = 0.2 might be a more reasonable assignment. 

*Specific assignments of probabilities must be consistent with reality in order for the probabilistic model to be useful.*

### Some Probability Laws

::: {.highlightbox data-latex=""}

-   $P(A) = 1 - P(A')$

-   $P(\emptyset) = 0$

-   If $A \subset B$, then $P(A) \leq P(B)$

-   $0 \leq P(A) \leq 1$

- $P(A \cup B) = P(A) + P(B) - P(A \cap B)$ (Addition rule)
:::

The last law is known as the "Additional Rule" 

Must avoid "double counting" the intersection of the two events

```{r, eval = FALSE, echo = FALSE, out.width="25%", fig.align="center"}
knitr::include_graphics("./images/Venn_AB.png")
```

**Special case**: If $A$ and $B$ are disjoint, then $P(A \cup B) = P(A) + P(B)$ because $P(A \cap B) = 0$

# 2.5 Calculating the Probability of an Event: The Sample-Point Method

For discrete sample spaces, it suffices to assign probabilities to each simple event. 

::: {.activitybox data-latex=""}

**Example**: Toss a balanced six-sided die once.

+ What is one possible simple event?

+ What is the sample space? 
\


+ What is a reasonable probability for each sample point in $S$? Confirm that the first two properties of Definition 2.6 hold.
\
\
\
\
\





:::

::: {.highlightbox data-latex=""}

**The sample-point method** for calculating probability

1. Define the experiment and clearly determine how to describe one simple event.
2. Define the sample space by listing the simple events of the experiment
3. Assign probabilities to the sample points in $S$, making certain that $P(E_i) \geq 0$ and $\sum P(E_i) = 1$
4. Define the event of interest, $A$, as a specific collection of sample points. 
5. Find $P(A)$ by summing the probabilities of the sample points in $A$. 

:::

::: {.activitybox data-latex=""}

**Example**:

A fair coin is tossed 3 times, and the sequence of heads and tails is observed. What is the probability that at least two heads is observed? 

\
\
\
\
\
\
\
\
\
\
\
\
:::

::: {.activitybox data-latex=""}

**Example (cont'd)**: Using the same scenario as above (three coin tosses), define the following additional events (A listed again for completeness): 

+ $A = \{\text{at least two heads}\}$
+ $B =  \{\text{at most two heads}\}$
+ $C = \{\text{heads on the 2nd toss}\}$
+ $D = \{\text{1 head and 2 tails}\}$

Find:

a) $P(B)$
\
\
b) $P(A \cap B)$
\
\
c) $P(A \cap C)$
\
\
d) $P(D)$
\
\
e) $P(A \cup C)$
\
\
f) $P(B \cap D)$

\
\
:::

The sample-point method is direct and powerful, but is a bit of a bulldozer approach. It can be cumbersome to list all the events, and it can be susceptible to human error. Orderliness, computers, and the mathematical theory of counting (combinatorics) can help reduce effort and human error in carrying out the sample-point method for calculating probabilities. 

# 2.6 Tools for Counting Sample Points

Combinatorics can help you calculate the total number of sample points in a sample space $S$ or an event of interest $A$, which can provide a confirmation (or avoidance) of your listing of simple events. Sometimes counting methods are the only way to calculate a probability if the sample space is very large. 

::: {.activitybox data-latex=""}

If a sample space $S$ contains $N$ equiprobable sample points and an event $A$ contains exactly $n_A$ sample points, then $$P(A) = \frac{n_A}{N}$$

:::


### Multiplication Rule or "$mn$" Rule

::: {.activitybox data-latex=""}

**Example**: A sandwich shop offers 3 types of bread and 4 meats. How many different sandwiches are possible?

> *Hint: think in terms of "slots" for each element of the sandwich, and ask how many ways to fill 1st slot, then 2nd etc.*

> *OR use a "TREE DIAGRAM" to enumerate all possibilities*

```{r, echo = FALSE, eval = FALSE, out.width="20%", out.height = "20%", fig.align="left"}
knitr::include_graphics("./images/1.2_tree_diagram.png")
```
\
\
\
\
\
\
\
\
\
\
\
\
:::

**General Rule**

::: {.highlightbox data-latex=""}

If procedure 1 has $m$ possible outcomes ($a_1, a_2, \dots a_m$), procedure 2 has $n$ possible outcomes ($b_1, b_2, \dots b_n$), it is possible to form $mn = m \times n$ composite outcomes, containing one element from each group. 

:::

This multiplication rule can be extended to any number of sets. 

::: {.activitybox data-latex=""}

**Example**: Refer back to the experiment of tossing 3 coins. Use the extension of the $mn$ rule to confirm the number of sample points. 

\
\
\

:::

::: {.activitybox data-latex=""}

Suppose you randomly select 20 people from a population and record their birthdays $(b_1, b_2, \dots ,b_{19},b_{20})$. Ignoring leap years and assuming there are only 365 possible distinct birthdays, find the number of points in the sample space $S$ for this experiment.

\
\
\
\
\
\ 
If we assume each of the possible sets of birthdays is equiprobable, what is the probability that each person in the 20 has a different birthday? 

*Hint: Define $A$ = set of all 20-tuples where there are no repeat numbers. To answer this question, we need to find the number of sample points in $A$*. 
\
\
\
\
\
\
\
\


:::

### Permutations

We've seen that sample points are often represented by a sequence of numbers or symbols (e.g., HHH). Often, it will be clear that the total number of sample points equals the number of distinct ways that the representative symbols can be arranged in sequence.

::: {.activitybox data-latex=""}

**Examples**

In how many ways can you arrange Art, Bud and Carl in 3 seats?

\
\
\


> In how many ways can you arrange Art, Bud, Carl and Dave in 4 seats?

\
\
\


> What if there are 10 people and 10 seats - how many arrangements?

> *Hint: think in terms of "slots" for the seats \_ \_ \_ \_ \_ \_ \_ \_ \_ \_ and ask how many ways to fill 1st seat, then 2nd given someone is in 1st etc.*

\
\
\
\

:::

\textcolor{red}{\textbf{Factorial}} = Total number of ways to order n (unique) subjects
$$n! = \text{"n factorial"} = n \times (n-1) \times \cdots \times 2 \times 1$$

::: {.activitybox data-latex=""}

**Example**:

What if there are 10 people and only 3 seats - how many arrangements?

> *Hint: think in terms of "slots" again...start with first slot*

\
\
\
\

:::

\textcolor{red}{\textbf{Permutation}} = an ordered arrangement of $r$ distinct objects. The number of ways of ordering $n$ distinct objects, taken $r$ at a time, is given by

$$_nP_r = n(n-1)(n-2)\cdots(n - r + 1)=\frac{n!}{(n-r)!}$$

In our example, 3 seats (slots) for 10 people: 10 x 9 x 8 x \_\_\_\_\_
we need to "remove" slots 7, 6, 5,...,1 from the factorial:

$$_{10}P_3 = \frac{10!}{(10-3)!} = \frac{10!}{7!} = \frac{10 \times 9 \times 8 \times \textcolor{red}{7 \times \cdots \times 1}}{\textcolor{red}{7 \times \cdots \times 1}} = 10 \times 9 \times 8$$


The Permutation formula $_nP_r = \frac{n!}{(n-r)!}$ is based on
**sampling WITHOUT replacement**

::: {.activitybox data-latex=""}

**Example:**

What if there are 10 people and only 3 seats -- but it is a RAFFLE of tickets to a show and each person can win more than one ticket (or all three). How many ways are there to choose 3 tickets in this scenario?

> *Hint: We choose 1st winner THEN REPLACE THAT NAME IN THE HAT before picking second winner.*

\
\
\
\
\
\

:::

For **sampling WITH replacement**: use $n^r$

$$n^3 = 10 \times 10 \times 10 = 1000$$

::: {.highlightbox data-latex=""}
**Theorem 2.3** The number of ways of partitioning $n$ distinct objects into $k$ distinct groups containing $n_1, n_2, \dots n_k$ objects, respectively, where each object appears in exactly one group and $\sum_{i = 1}^k n_i = n$ is 

$$N = \binom{n}{n_1 \ \ n_2 \ \cdots \ \ n_k} = \frac{n!}{n_1! n_2! \cdots n_k!}$$

:::

::: {.activitybox data-latex=""}

**Example**: A labor dispute has arisen concerning the distribution of 20 laboreres to four different construction jobs. The first job (considered to be very undesireable) requires 6 laborers; the other three jobs required 4, 5, and 5 laborers respectively. The dispute arose over an alleged random distribution of the laborers to jobs that placed all 4 members of a particular ethnic group on job 1. In considering whether the assignment represented injustice, a mediation panel desired the probability of the observed event. Determine the number of sample points in the sample space $S$ for this experiment. That is, determine the number of ways the 20 laborers can be divided into groups of the appropriate sizes to fill all the jobs. Then, find the probability of the observed event if it is assumed that the laborers are randomly assigned to jobs.
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
:::

### Combinations

Sometimes the *order* of sample points in a subset is unimportant, and all that matters is which sample points are chosen. 

::: {.activitybox data-latex=""}

**Example**: What if there are 10 people and 3 identical prizes - how many
arrangements of prize winners if we don't care about order (i.e we just want to pick 3 people)?

> *Hint: for one set of 3 people, how many extra sets do we get if order mattered? That is, think of how many ways to order the letters ABC*

\
\
\
\

:::

\textcolor{red}{\textbf{Combination}} = Total number of ways pick to r (unique) subjects selected from n subjects (also known as a "distinguishable permutation")

$$_nC_r = \frac{_nP_r}{\textcolor{blue}{r!}} = \frac{n!}{(n - r)! \textcolor{blue}{r!}}$$
We "divide out" the extra cases where order changes. In our example, choosing 3 of 10 people for each combination of 3 we get 3! "extra" if we allow order to matter. Example, combination ABC:
\textcolor{blue}{ABC, \ ACB, \ BAC, \ BCA, \ CAB, \ CBA = 3! = \text{6 times counted in Permutation}}
$$_{10}C_3 = \frac{10!}{(10 - 3)! \textcolor{blue}{3!}} = \frac{10 \times 9 \times 8}{\textcolor{blue}{6}}$$
Helpful to "read" the mathematical notation
$$_nC_r = \text{"n choose r"}$$

::: {.activitybox data-latex=""}

Example: Find the number of ways of selecting two applicants out of five.
\
\
\
\
\
\

:::

::: {.activitybox data-latex=""}

Example: How many ways are there to have a 3 daughters among a family of 5 children? In other words, how many ways can 3 "Fs" be placed in 5 "slots"? 
\
\
\
\
\
\

:::

\pagebreak 

### Binomial Coefficients & Pascal's triangle

$$_nC_r = \binom{n}{r} = \frac{n!}{(n-r)!r!}$$
```{r, echo = FALSE, fig.align="center", out.width="70%"}
knitr::include_graphics("./images/pascals_triangle.png")
```

```{r, echo = FALSE, eval = FALSE, fig.align="center", out.width="50%"}
knitr::include_graphics("./images/PascalTriangleAnimated.gif")
```

**Video on Pascal's triangle:** [https://ed.ted.com/lessons/the-mathematical-secrets-of-pascal-s-triangle-wajdi-mohamed-ratemi](https://ed.ted.com/lessons/the-mathematical-secrets-of-pascal-s-triangle-wajdi-mohamed-ratemi)

**Seeing Theory website:** [https://seeing-theory.brown.edu/compound-probability/index.html#section2](https://seeing-theory.brown.edu/compound-probability/index.html#section2)


# 2.9 Calculating the Probability of an Event: The Event-Composition Method

We've seen that sets (events) can often be expressed as unions, intersections, or complements of other events. The event-composition method for calculating the probability of an event $A$, expresses $A$ as a composition involving unions and/or intersections of other events. The laws of probability are then applied to find $P(A)$. 

::: {.activitybox data-latex=""}

**Example**: Of a group of patients having injuries, 28% visit both a physical therapist and a chiropractor and 8% visit neither. Say that the probability of visiting a physical therapist exceeds the probability of visiting a chiropractor by 16%. What is the probability of a randomly selected person from this group visiting a physical therapist?

\
\
\
\
\
\
\
\
\
\

:::

# 2.7 Conditional Probability and the Independence of Events

::: {.activitybox data-latex=""}

**Example:**

Roll two dice. Let Event B = the dice add to 4.

```{r, echo = FALSE, out.width="50%", out.height = "10%", fig.align="center"}
knitr::include_graphics("./images/dice_sample_space.png")
```

> **Q1**: Before rolling the dice, what is P(B)?

\


> **Q2**: Suppose I roll one dice and get a 1 (i.e. Event A = get a 1 on first dice). What is the probability of B now that A has occured?

\


:::

In Q2, we wanted the \textcolor{red}{\textbf{conditional probability}} - the probability that B will occur given A has already occurred.

$$P(B|A) = \text{probability of }B  \textbf{ given } A$$

Rolling a 1 reduced the sample space from 36 possible outcomes to 6:

```{r, echo = FALSE, out.width="50%", out.height = "10%", fig.align="left"}
knitr::include_graphics("./images/dice_conditional.png")
```

With only 1 of these being a sum of 4 we get:

$$P(B|A) = \frac{1}{6}$$

::: {.activitybox data-latex=""}

What does Conditional Probability look like in a Venn Diagram?

```{r, echo = FALSE, out.width="30%", out.height = "10%", fig.align="center"}
knitr::include_graphics("./images/Venn_AB.png")
```

:::

::: {.highlightbox data-latex=""}

The \textcolor{red}{\textbf{conditional probability}} of an event B,
given that event A has occured, is defined by:

$$P(B|A) = \frac{P(A \cap B)}{P(A)} = \frac{P(A \text{ and } B)}{P(A)}$$

:::

Note that $P(A) > 0$

::: {.highlightbox data-latex=""}

Conditional probabilities satisfy the axioms of Probability:

-   $P(B|A) \geq 0;$
-   $P(A|A) = 1;$
-   $P(B'|A) = 1 - P(B|A)$
-   if $B_1, B_2, ... B_k$ are mutually exclusive events then
    $$P(B_1 \cup B_2 \cup ... \cup B_k | A) = P(B_1 | A) + P(B_2|A) + ... + P(B_k|A)$$

:::

```{r, echo = FALSE, out.width="30%", out.height = "30%", fig.align="center"}
knitr::include_graphics("./images/cond_prob_serial_killer.png")
```

\pagebreak 

### Independence Day Activity (courtesy of Allan Rossman)

::: {.activitybox data-latex=""}

In 2025, the New York times released a list of the 100 best movies from the 20th century so far. See their list [here](https://www.nytimes.com/interactive/2025/movies/best-movies-21st-century.html). Below is some real data of which of the 100 movies my sister and I have seen.

|  | Anna has seen the film       | Anna has NOT seen the film | Total |
|----------------------|--------------------|--------------|-------- |
| Katie has seen the film | 42 | 6 | 48 |
| Katie has NOT seen the film | 17 |35 |52 |
| Total | 59 | 41 | 100|

Suppose a movie is chosen at random (meaning each movie is equally likely to be chosen) 

a) What is the probability that Anna has seen the film?
\
\
b) Given the partial information that Katie has seen the film, what is the updated (conditional) probability that Anna has seen it?
\
\
\

c) Does learning that Katie has seen the randomly selected film change the probability that Anna has seen it?  In which direction?  Why might this make sense?
\
\
\
\
d) Repeat this analysis based on the following table (of made-up data) for two other people, Nathalia and Joe:

|  | Joe has seen the film       | Joe has NOT seen the film | Total |
|----------------------|--------------------|--------------|-------- |
| Nathalia has seen the film | 42 | 28 | 70 |
| Nathalia has NOT seen the film | 18 |12 |30 |
| Total | 60 | 40 | 100|
\
\
\
\
\
\
\

e) In which case (Katie-Anna) or (Nathalia-Joe) would it make sense to say that the events are *independent*?
\
\

f) Based on these data, would you still say that (Katie having seen the film) and (Anna having seen the film) are dependent events, even if they never saw any films together and perhaps did not even know each other?
\
\
\
\

:::

::: {.highlightbox data-latex=""}

Statistical \textcolor{red}{\textbf{Independence}} = the occurance of
another event (A) has **no effect** on the probability of B occurring:
$$P(B|A) = P(B)$$ 

:::

::: {.activitybox data-latex=""}

Suppose we roll two fair, six-sided die. Let's assume one is red and the other is green, so we can tell them apart. Consider these four events: 

+ A = {green die lands on 6}
+ B = {red die lands on 5}
+ C = {sum equals 11}
+ D = {sum equals 7}. 

For each pair of events, determine whether or not the events are independent.  Justify your answers with appropriate probability calculations.
\
\
\
\
\
\
\
\
\
\
\
\

::: 




Recall that

$$\frac{P(A \cap B)}{P(A)} = P(B|A) \implies P(A \cap B) = P(B|A)\times P(A)$$

IF A and B are independent, this becomes:

$$P(A \cap B) = P(B|A)\times P(A) = P(B) \times P(A)$$

::: {.highlightbox data-latex=""}
**Definition 2.10**     Events $A$ and $B$ are said to be
\textcolor{red}{\textbf{independent}} if and only if any of the following holds:

+ $P(A\cap B) = P(A)P(B)$ 
+ $P(A|B) = P(A)$
+ $P(B|A) = P(B)$

Otherwise, $A$ and $B$ are called \textcolor{red}{\textbf{dependent}} events.
:::

### A few theorems & laws of probability

::: {.highlightbox data-latex=""}
**Theorem 2.5: The Multiplicative Law of Probability** The probability of the intersection of two events A and B is

$$\begin{aligned}
P(A \cap B) &= P(A) P(B|A) \\
&= P(B)P(A|B)
\end{aligned}$$

If A and B are independent, then

$$P(A \cap B) = P(A)P(B)$$
:::

::: {.highlightbox data-latex=""}
If $A$ and $B$ are independent events, then the
following pairs of events are also independent:

(a) $A$ and $B'$  
(b) $A'$ and $B$  
(c) $A'$ and $B'$
:::

PROOF:

\
\
\
\
\
\
\
\
\
\
\
\
\


::: {.activitybox data-latex=""}

**Example:** Suppose that you have applied to two internship programs E and F.  Based on your research about how competitive the programs are and how strong your application is, you believe that you have a 60% chance of being accepted for program E and an 80% chance of being accepted for program F.  Assume that your acceptance into one program is independent of your acceptance into the other program.

a) What is the probability that you will be accepted by both programs?
\
\

b) What is the probability that you will be accepted by at least one of the two programs?  Show two different ways to calculate this.
\
\
\
\
c) Suppose you also apply to programs G and H, and you believe you have a 70% and 20% chance of being accepted into each program, respectively. What is the probability that you will be accepted by all four programs?  Is this pretty unlikely?
\
\
\
\
d) What is the probability that you will be accepted by at least one of the four programs?  Is this very likely?
\
\
\
\
e) Explain why the assumption of independence is probably not reasonable in this situation.
\
\
\
\

:::

\pagebreak

::: {.activitybox data-latex=""}

Suppose that every day you play a lottery game in which a three-digit number is randomly selected.  Your probability of winning for each day is 1/1000.

a) Is it reasonable to assume that whether you win or lose is independent from day to day?  Explain.
b) Determine the probability that you win at least once in a 7-day week.  Report your answer with five decimal places.  Also explain why this probability is not exactly equal to 7/1000.
c) Determine the probability that you win at least once in a 365-day year.
d) Suppose that your friend says that because there are only 1000 three-digit numbers, you’re guaranteed to win once if you play for 1000 days.  How would you respond?
e) Express the probability of winning at least once as a function of the number of days that you play.  Also produce a graph of this function, from 1 to 3652 days (about 10 years).  Describe the function’s behavior.
f) For how many days would you have to play in order to have at least a 90% chance of winning at least once?  How many years is this?
g) Suppose that the lottery game costs $1 to play and pays $500 when you win.  If you were to play for that many days (your answer to the previous part), is it likely that you would end up with more or less money than you started with?
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\

:::


# 2.10 Law of Total Probability & Bayes Rule

<!-- RE-DO THIS WITH EXAMPLES FROM AGQ: https://askgoodquestions.blog/2020/10/19/68-knowing-or-guessing/ -->

::: {.activitybox data-latex=""}

**Example:**

*TRUE STORY (from a former colleague)*

After giving blood one year, Dr. S. received a letter saying the blood was rejected due to a positive test result for a Hepatitus A virus. The letter further stated this was "nothing to worry about"...?!? Suppose some research reveals that only 1% of people in the U.S. have HepA. The blood screening test is known to be correct 99% of the time when the person has the disease and 95% when the person does not. Should Dr. S. have worried? 

\

Define (two) events: 

\
\
\

What probabilities do we know? 

\
\
\

What probabilities do we want to know? 

\
\

Create a tree diagram:

\
\
\
\
\
\
\
\
\
\
\
\
\
\

:::

### Bayes' Rule

$$P(A|B) = \frac{P(A \cap B)}{P(B)} = \frac{P(B|A)P(A)}{P(B)}$$

::: {.highlightbox data-latex=""}
**Definition 2.10:** For some positive integer $k$, let the sets $B_1, B_2, \dots B_k$ be such that

1. $S = B_1 \cup B_2 \cup \dots \cup B_k$
2. $B_i \cap B_j = \emptyset$, for $i \neq j$

Then the collection of sets $\{B_1, B_2, \dots B_k\}$ is said to be a **partition** of $S$

:::

::: {.highlightbox data-latex=""}

**Theorem 2.8**: Assume that $\{B_1, B_2, \dots B_k\}$ is a partition of $S$ such that $P(B_i) > 0$ for $i = 1,2, \dots, k$. Then for any event A

$$P(A) = \sum_{i = 1}^kP(A|B_i)P(B_i)$$

:::

::: {.highlightbox data-latex=""}

**Theorem 2.9** (Bayes' Rule, more generally): Assume that $\{B_1, B_2, \dots B_k\}$ is a partition of $S$ such that $P(B_i) > 0$ for $i = 1,2, \dots, k$. Then

$$P(B_j|A) = \frac{P(A|B_j)P(B_j)}{P(A)}=\frac{P(A|B_j)P(B_j)}{\sum_{i = 1}^kP(A|B_i)P(B_i)}$$
:::

+ Numerator is simply re-writing the intersection REVERSING THE CONDITIONAL PROBABILITY
+ Denominator is known as the **Law of Total Probability**

### Bayes' Terminology

\textcolor{red}{\textbf{Prior probability - }}$P(B_j)$ the probability BEFORE we observe information $A$

\textcolor{red}{\textbf{Posterior probability - }}$P(B_j|A)$ the probability AFTER we observe A ... "updated" probability of the event
based on "new data/information"


\pagebreak

# Chapter 2 Group Work Practice

1. How many four-letter code words are possible using the letters in IOWA if:

    a. The letters may not be repeated?
    b. The letters may be repeated? 

2. Three students (S) and six faculty members (F) are on a panel discussing a new college policy.

    a. In how many different ways can the nine participants be lined up at a table in front of the auditorium?
    a. How many lineups are possible if you only care about where the students are placed in relation to faculty? In other words, how many ways are there to choose 3 seats to place the students in (or alternatively, choose 6 seats to place the faculty in)?
    a. For each of the nine participants, you are to decide whether the participant did a good job or a poor job stating their opinion of the new policy; that is, give each of the nine participants a grade of G or P. How many different "scorecards" are possible?
    
3. The NBA Finals continues until either the Western Conference team or the Eastern Conference team wins four games. How many different orders are possible (e.g. WEEWWW means the Western Conferences team wins in six games) if the series lasts....
    
    a. four games? 
    a. five games? *Hint: start by focusing on the number of ways the Western Conference can win in 5. And note that if the West wins in 5, the results are of the form _ _ _ _ W*
    a. six games?
    a. seven games?

4. Suppose that 78% of the students at a particular college have a TikTok account and 43% have a Twitter account. Using only this information, 

    a. what is the largest possible value for the percentage who have both a TikTok account and a Twitter account?  Describe the (unrealistic) situation in which this occurs.
    b. what is the smallest possible value for the percentage who have both a TikTok account and a Twitter account?  Describe the (unrealistic) situation in which this occurs.
    
| | Twitter | No Twitter | Total|
|----|-----|-----|------|
TikTok | | | 0.78
No TikTok | | | 0.22
Total | 0.43 | 0.57 | 1.00
    
 > Now assume you know that 36% of students have both a TikTok and a Twitter account.
    
> c. What percentage of students have at least one of these accounts?
> d. What percentage of students have neither of these accounts?
> e. What percentage of students have one of these accounts but not both?

5. Let $A_1$ and $A_2$ be the events that a person is left- eye dominant or right-eye dominant, respectively. When a person folds their hands, let $B_1$ and $B_2$ be the events that the left thumb and right thumb, respectively, are on top. A survey in one statistics class yielded the following table:

| | $B_1$ | $B_2$ | Totals |
|-----|---|---|-----|
|$A_1$ | 5 | 7 | 12 |
| $A_2$ | 14 | 9 | 23 |
| Totals | 19 | 16 | 35 |

> If a student is selected randomly, find the following probabilities:	
> (a)	$P(A_1 \cap B_1)$,	
> (b) $P(A_1 \cup B_1)$,    
> (c) $P(A_1|B_1)$,   
> (d) $P(B_2 | A_2)$.   
> (e) If the students had their hands folded and you hoped to select a right-eye-dominant student, would you select a “right thumb on top” or a “left thumb on top” student? Why?

6. Let $A$ and $B$ be independent events. Prove that $A'$ and $B$ are also independent. 

7. Two processes of a company produce rolls of materials: The rolls of Process I are 3% defective and the rolls of Process II are 1% defective. Process I produces 60% of the company’s output, Process II 40%. A roll is selected at random from the total output. Given that this roll is defective, what is the conditional probability that it is from Process I?

```{r, echo = FALSE, eval = FALSE}
data.frame(c("$A_1$", "$A_2$", "Totals"), 
           b1 = c(5, 15, 19), 
           b2 = c(7, 9, 16), 
           Totals = c(12,23,35),
           row.names = 1) %>% 
  kable(col.names = c("$B_1$", "$B_2$", "Totals"))
#
```


<!-- ::: {.activitybox data-latex=""} -->

<!-- **Example:** -->

<!-- An urn contains four balls numbered 1, 2, 3, and 4. One ball is to be drawn at random from the urn. Let the events $A$, $B$, and $C$ be defined by $A = \{1,2\}$, $B = \{1,3\}$, and $C = \{1, 4\}$. Then $P(A) = P(B) = P(C) = 1/2$. Are pairs of events $AB, AC \text{ and } BC$ independent? -->

<!-- \ -->
<!-- \ -->
<!-- \ -->
<!-- \ -->
<!-- \ -->
<!-- \ -->
<!-- \ -->
<!-- \ -->
<!-- \ -->
<!-- \ -->
<!-- \ -->
<!-- \ -->
<!-- \ -->

<!-- ::: -->

<!-- ## Mutual Independence -->

<!-- ::: {.highlightbox data-latex=""} -->

<!-- **Definition 1.4-2**     Events $A$, $B$, and $C$ are -->
<!-- \textcolor{red}{\textbf{mutually independent}} if and only if the -->
<!-- following two conditions hold: -->

<!-- 1.  $A$, $B$, and $C$ are pairwise independent; that is, -->
<!--     $$P(A\cap B) = P(A)P(B),$$ $$P(A\cap C) = P(A)P(C),$$ and -->
<!--     $$P(B\cap C) = P(B)P(C)$$ -->
<!-- 2.  $P(A \cap B \cap C) = P(A)P(B)P(C).$ -->

<!-- ::: -->

<!-- ::: {.activitybox data-latex=""} -->


<!-- **Example (cont'd)**: -->

<!-- Are the events $A$, $B$, and $C$ in the urn Example mutually -->
<!-- independent? -->

<!-- \ -->
<!-- \ -->
<!-- \ -->
<!-- \ -->
<!-- \ -->
<!-- \ -->

<!-- ::: -->

<!-- ::: {.activitybox data-latex=""} -->

<!-- **Example: flipping 2 coins** -->

<!-- > Event A = heads on 1st flip -->

<!-- > Event B = tails on second flip -->

<!-- > Event C = tails on both flips -->

<!-- Define the events above and then compute the following probabilities -->

<!-- -   $P(B)$  -->
<!-- -   $P(B|C)$  -->
<!-- -   $P(B|A)$ -->

<!-- \ -->
<!-- \ -->
<!-- \ -->
<!-- \ -->
<!-- \ -->
<!-- \ -->

<!-- ::: -->

<!-- ::: {.activitybox data-latex=""} -->

<!-- **Example:** -->

<!-- A hospital receives two fifths of its flu vaccine from Company A and the remainder from Company B. Each shipment contains a large number of vials of vaccine. From Company A, 3% of the vials are ineffective; from Company B, 2% are ineffective. A hospital tests $n = 25$ randomly selected vials from one shipment and finds that 2 are ineffective. What is the conditional probability that this shipment came from Company A? -->

<!-- > *Hint: define C to be the event that 2/25 vials are ineffective* -->

<!-- \ -->
<!-- \ -->
<!-- \ -->
<!-- \ -->
<!-- \ -->
<!-- \ -->
<!-- \ -->
<!-- \ -->
<!-- \ -->
<!-- \ -->
<!-- \ -->
<!-- \ -->
<!-- \ -->
<!-- \ -->

<!-- ::: -->

<!-- ## Let's Make a Deal (The Monty Hall Problem) -->

<!-- ::: {.activitybox data-latex=""} -->

<!-- There are 3 doors, one with a big prize behind it. You picked one of 3 doors. The dealer then revealed one of the doors you did not selected...not the big prize. He offers you the chance to keep your door or change to the remaining door. **Should you switch doors?** -->
<!-- \ -->
<!-- Without loss of generality, assume you choose door A. Use Bayes' theorem to calculate the probability the BIG PRIZE is behind your door. What is the best strategy? When the host reveals a goat behind B (or C), should you switch to C (or B), or stick with A?  How does the dealer opening one door update the probability for each door? -->
<!-- \ -->
<!-- *Hint: Set up a tree diagram with the three doors (and their probability of containing the prize) as the first set of branches, and the hosts' choice "B" or "C" as the second set of branches. Then, determine how to represent P(Win|Stay) and P(Win|Switch) in terms of these events A, B, C.*  -->
<!-- \ -->
<!-- \ -->
<!-- \ -->
<!-- \ -->
<!-- \ -->
<!-- \ -->
<!-- \ -->
<!-- \ -->
<!-- \ -->
<!-- \ -->
<!-- \ -->
<!-- \ -->
<!-- \ -->
<!-- \ -->
<!-- \ -->
<!-- \ -->
<!-- \ -->
<!-- \ -->
<!-- \ -->
<!-- \ -->

<!-- ::: -->